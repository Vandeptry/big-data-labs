version: "3.8"

services:
    # ============================
    #        SPARK CLUSTER
    # ============================
    spark-master:
        image: bde2020/spark-master:3.3.0-hadoop3.3
        container_name: spark-master
        hostname: spark-master
        ports:
            - "8080:8080"
            - "7077:7077"
        environment:
            - INIT_DAEMON_STEP=setup_spark
        volumes:
            - ./src:/app/src
            - ./data:/data
        networks:
            - bigdata_network

    spark-worker1:
        image: bde2020/spark-worker:3.3.0-hadoop3.3
        container_name: spark-worker-1
        depends_on:
            - spark-master
        ports:
            - "8081:8081"
        environment:
            - SPARK_MASTER=spark://spark-master:7077
        volumes:
            - ./src:/app/src
            - ./data:/data
        networks:
            - bigdata_network

    spark-worker2:
        image: bde2020/spark-worker:3.3.0-hadoop3.3
        container_name: spark-worker-2
        depends_on:
            - spark-master
        ports:
            - "8082:8081"
        environment:
            - SPARK_MASTER=spark://spark-master:7077
        volumes:
            - ./src:/app/src
            - ./data:/data
        networks:
            - bigdata_network

    spark-client:
        image: eclipse-temurin:17-jdk-focal
        container_name: spark-client
        depends_on:
            - spark-master
            - namenode
        volumes:
            - ./src:/app/src
            - ./data:/data
        environment:
            - "CORE_CONF_fs_defaultFS=hdfs://namenode:8020"
            - SPARK_MASTER_URL=spark://spark-master:7077
        networks:
            - bigdata_network
        command: >
            /bin/bash -c "apt-get update &&
            apt-get install -y python3 python3-pip wget curl &&
            pip install pyspark &&
            exec /bin/bash"
        tty: true

    # ============================
    #        HADOOP CLUSTER
    # ============================
    namenode:
        image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
        container_name: namenode
        hostname: namenode
        ports:
            - "9870:9870"
            - "8020:8020"
        volumes:
            - hadoop_namenode:/hadoop/dfs/name
            - ./data:/data
        environment:
            - CLUSTER_NAME=test
            - "CORE_CONF_fs_defaultFS=hdfs://namenode:8020"
        networks:
            - bigdata_network

    datanode1:
        image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
        container_name: datanode-1
        depends_on:
            - namenode
        volumes:
            - hadoop_datanode1:/hadoop/dfs/data
            - ./data:/data
        environment:
            - SERVICE_PRECONDITION=namenode:9870
            - "CORE_CONF_fs_defaultFS=hdfs://namenode:8020"
        networks:
            - bigdata_network

    datanode2:
        image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
        container_name: datanode-2
        depends_on:
            - namenode
        volumes:
            - hadoop_datanode2:/hadoop/dfs/data
            - ./data:/data
        environment:
            - SERVICE_PRECONDITION=namenode:9870
            - "CORE_CONF_fs_defaultFS=hdfs://namenode:8020"
        networks:
            - bigdata_network

    datanode3:
        image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
        container_name: datanode-3
        depends_on:
            - namenode
        volumes:
            - hadoop_datanode3:/hadoop/dfs/data
            - ./data:/data
        environment:
            - SERVICE_PRECONDITION=namenode:9870
            - "CORE_CONF_fs_defaultFS=hdfs://namenode:8020"
        networks:
            - bigdata_network

networks:
    bigdata_network:
        driver: bridge

volumes:
    hadoop_namenode:
    hadoop_datanode1:
    hadoop_datanode2:
    hadoop_datanode3:
